# Week-2
In week 2 we made prompts and ran it in different models.
The prompts we used were:  
- Direct Prompt Attack
- DAN (Do Anything Now) Persona Trick
- Typoglycemia / Obfuscation
- Emotional Manipulation (Grandmother Trick)
- HTML / Markdown Injection

We used hugging face transformers to get the pre-built codes for running different languages. Pipeline is used to access the pre-assembled machine.

Each prompt was run on all three models, outputs were recorded, and detailed tables were created for each model in the respective Markdown files.  
Results demonstrated that instruction-tuned models are more responsive — and thus more jailbreak-sensitive — compared to non-aligned models.
