# Jail-Breaking-Attacks-on-Large-Language-Models-Techniques-and-Effectiveness
This project explores various jailbreaking attacks on large language models, demonstrating how carefully  crafted prompts can bypass safety mechanisms. Techniques such as persona-based attacks, obfuscation,  and emotional manipulation will be implemented and tested on open-source LLMs.
