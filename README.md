# Jail-Breaking-Attacks-on-Large-Language-Models-Techniques-and-Effectiveness
This project explores various jailbreaking attacks on large language models, demonstrating how carefully  crafted prompts can bypass safety mechanisms. Techniques such as persona-based attacks, obfuscation,  and emotional manipulation will be implemented and tested on open-source LLMs.

# Software requirements:
- Operating System: Windows 10/11 
- Programming Language: Python 3.9+ 
- Drivers: GPU drivers                                            
- Tools: Jupyter Notebook, Google Colab, Hugging Face Transformers, LangChain , Pandas, Matplotlib 

# Hardware Requirements:  
- Processor: Intel i5/i7 
- Memory: Minimum 8 GB RAM 
- Any other devices: no(except for the group members)
